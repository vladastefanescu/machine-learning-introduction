{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine Learning Introduction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vladastefanescu/machine-learning-introduction/blob/main/Machine_Learning_Introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SQCVuhLAgJ6"
      },
      "source": [
        "# <center>Performance Evaluation in Machine Learning</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3Acp-WI-n9Q"
      },
      "source": [
        "## 1. Introduction\n",
        "\n",
        "Just like chess players improve their technique by watching hundreds of games from top players, computers can be able to perform certain tasks by ‚Äúlooking‚Äù at data. These computers are sometimes called ‚Äú**machines**‚Äù and this data observation step is also known as ‚Äú**learning**‚Äù. Simple, isn‚Äôt it? So let‚Äôs define ‚Äú**Machine Learning**‚Äù (ML) more formally, as stated by Tom Mitchell. A program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E. Let‚Äôs get more **-h** and provide an example. Say that T is the process of playing chess, E is a set of chess matches on which the algorithm was trained and P is the probability for the program to win the next game of chess.\n",
        "\n",
        "What is truly important however, is that machine learning algorithms, unlike traditional ones, are not required to be pre-programmed explicitly to solve a particular task. Just like in the previous example, they **learn** from the data fed into them and afterwards, are able to **predict** results for new scenarios. As you might expect, this isn‚Äôt just black magic. The majority of ML algorithms are based on some maths and statistics, but also on a fair amount of engineering. üòé \n",
        "\n",
        "<br/>\n",
        "\n",
        "![machine-learning-flow](https://miro.medium.com/max/1400/1*E7sk9C5buY9KnBUDflI9fQ.png)\n",
        "[üåé &nbsp;Image Source](https://medium.com/@tekaround/train-validation-test-set-in-machine-learning-how-to-understand-6cdd98d4a764)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qb_cHSUBdGz"
      },
      "source": [
        "---\n",
        "> ### üö®üö®üö® Machine learning algorithms learn from data and they **don‚Äôt** have to be programmed explicitly!\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLzfz9DaExIc"
      },
      "source": [
        "## 2. Supervised Learning vs Unsupervised Learning\n",
        "\n",
        "Generally, in machine learning, there are two types of tasks: **supervised** and **unsupervised**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6R1yaPwLCWnu"
      },
      "source": [
        "### A. Supervised Learning\n",
        "\n",
        "We refer to the former when we have prior knowledge of what the output of the system should look like. More formally, after going through lots of `(X, y)` pairs, the model should be able to determine that **mapping function** `ƒ•` which approximates `f(X) = y` as accurately as possible. In machine learning, the entire set of `(X, y)` pairs is often called **corpus** or **dataset** and an algorithm that learned from this corpus is sometimes called a **trained model** or simply a **model**.\n",
        "\n",
        "For example, let‚Äôs say that you want to build a smart car selling platform and you want to provide sellers with the ability to know what should be a ‚Äúfair price‚Äù based on the **specs** and **age** of their cars. For that, you would need to provide your model with a set of tuples that might look like this:\n",
        "\n",
        "`(bhp, fuel_type, displacement, weight, torque, suspension, car_age, car_price)`\n",
        "\n",
        "In this case, **X** is formed from the tuple slices `(bhp, fuel_type, displacement, weight, torque, suspension, car_age)` and each slice component is called a **feature**. Likewise, **y** is represented by the `car_price` and it's usually called **label** or **ground truth**. After learning from the provided corpus, a process that is commonly called **model fitting**, our model should be able to predict an approximation of the car price `≈∑` for new ages and specs **X**.\n",
        "\n",
        "\n",
        "<br/>\n",
        "\n",
        "![supervised-learning](https://bigdata-madesimple.com/wp-content/uploads/2018/02/Machine-Learning-Explained1.png)\n",
        "\n",
        "[üåé &nbsp;Image Source](https://bigdata-madesimple.com/machine-learning-explained-understanding-supervised-unsupervised-and-reinforcement-learning/)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfCeTi7XtboS"
      },
      "source": [
        "---\n",
        "> ### üö®üö®üö® In the dataset of each **supervised** machine **learning** task, there will always be a **label y** that can be associated with a set of **features X**!\n",
        "---\n",
        "\n",
        "Because a **numerical** or **continuous** output is expected as a result, the aforementioned scenario is an example of a **regression task**. If y was a label that can take the values ‚Äúcheap‚Äù, ‚Äúfair‚Äù or ‚Äúexpensive‚Äù, then that would be called a **classification task** and y can also be referred to as a **class**.\n",
        "\n",
        "<br/>\n",
        "\n",
        "![classification-vs-regression](https://ocw.cs.pub.ro/courses/_media/ep/labs/3._classification_vs_regression.png?w=600&tok=f48ebb)\n",
        "\n",
        "[üåé &nbsp;Image Source](https://towardsdatascience.com/do-you-know-how-to-choose-the-right-machine-learning-algorithm-among-7-different-types-295d0b0c7f60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtDdxuKjtVyY"
      },
      "source": [
        "---\n",
        "> ### üö®üö®üö® Two examples of **supervised learning** tasks are **regression** and **classification**!\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Xa5b23tGfSG"
      },
      "source": [
        "### B. Unsupervised Learning\n",
        "\n",
        "In the case of **unsupervised learning**, the training data is not labeled anymore, so the machine learning algorithm must take decisions without having a ground truth as reference. That means that your dataset is no longer made out of (X, y) pairs, but only of X entries. Consequently, the job of the model would be to provide meaningful **insights** about those X entries, by finding various **patterns** in the data.\n",
        "\n",
        "<br/>\n",
        "\n",
        "![unsupervised-learning](https://miro.medium.com/max/977/0*1F-c2hszf8NX-7Ku)\n",
        "\n",
        "[üåé &nbsp;Image Source](https://medium.com/@rahulkaliyath/day-8-intro-to-unsupervised-learning-3821ef1541a)\n",
        "\n",
        "The most common tasks for this learning approach are **clustering algorithms**. In these type of problems, the model must learn how to group various data together, forming **components** or **clusters**. Say that having a table with information about all the Facebook users, you would like to recommend personalized ads for every single person. This sounds like a very difficult task, because there is a huge number of users on the platform. But what if we group users together based on their **traits** (or **features** as your Data Scientist within you might say)? Then, we can suggest the same ads bundle to a whole group of users, making the advertising process easier and cheaper.\n",
        "\n",
        "![unsupervised-learning-example](https://miro.medium.com/max/700/1*af-tNiqd-3_ResjoDOFm5A.png)\n",
        "\n",
        "[üåé &nbsp;Image Source](https://towardsdatascience.com/introduction-to-machine-learning-for-beginners-eed6024fdb08)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ey7XBkHutz6B"
      },
      "source": [
        "---\n",
        "> ### üö®üö®üö® In the dataset of each **unsupervised** machine **learning** task, there will exist **only X** entries used to identify **patterns** and **insights** about data!\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IxeJKeiITRq"
      },
      "source": [
        "## 3. Performance Evaluation in Machine Learning\n",
        "\n",
        "Generally, with machine learning algorithms, in the early phase of development, the focus is less on the computational resources (RAM, CPU, IO etc.) and more on the ability of the model to **generalize** well on the data. That means that our algorithm is trained well enough to provide accurate answers for data that has never been seen before (predict a car price for a new car posted on the platform, classify a new vehicle or assign a freshly registered Facebook user to a group). Hence, at first, we try to build a robust and accurate model and then we work towards making it function as computationally inexpensive as possible.\n",
        "\n",
        "---\n",
        "> ### üö®üö®üö® In machine learning, traditional evaluation metrics (RAM, CPU, IO) are not used as frequently!\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQVgt9h5I5lc"
      },
      "source": [
        "### A. Training & Test Sets\n",
        "\n",
        "Evaluating machine learning algorithms is dependent on the type of problem we are trying to solve. But in most cases, we would want to split the data into a **training set** and a **test set**. As the name suggests, the former will be used to actually train the model and the later will be used to verify how well the model generalizes on unseen data. On the most common machine learning tasks, the split ratio ranges from **80-20** to **90-10**, depending on the size of the corpus. When a huge amount of data is operated (say millions or billions of entries), a smaller proportion of the dataset is used as a test set (even 1%) because the actual number of test entries are considered sufficiently numerous. For instance, 1% of 10M entries is 100k and that‚Äôs generally regarded as **a lot of data** to test on.\n",
        "\n",
        "![training-set-and-test-set](https://ocw.cs.pub.ro/courses/_media/ep/labs/6._training_and_test_sets.png?w=600&tok=2d3bd9)\n",
        "\n",
        "[üåé &nbsp;Image Source](https://data-flair.training/blogs/train-test-set-in-python-ml/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRlgJASst-yX"
      },
      "source": [
        "---\n",
        "> ### üö®üö®üö® In most problems, the data is split into a **training set** used in model training and a **test set** used in performance evaluation!\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2s2WEJ63KFDO"
      },
      "source": [
        "### B. Classification Problems\n",
        "\n",
        "Let‚Äôs begin our performance analysis journey by analyzing a **classification problem**. Let‚Äôs suppose that you have a model that was trained to predict whether a given image corresponds to a cat or not. As you already know, this is a **supervised learning** task in which the model must learn to **predict** one of **two classes** - ‚Äúcat‚Äù or ‚Äúnon-cat‚Äù. In this case, the model is also called a **binary classifier**.\n",
        "\n",
        "![brittish-shorthair-kitten](https://images.squarespace-cdn.com/content/v1/525a1ae4e4b0b7382f191ee4/1527746265982-2PIBO5N32RF1WKTYACYI/British-Shorthair-kitten-cute-squat-3000x2000.jpg?format=1500w)\n",
        "\n",
        "[üåé &nbsp;Image Source](http://www.british-shorthair-kitten.com/)\n",
        "\n",
        "#### Confusion Matrix\n",
        "\n",
        "You fit the model with your training data, you ask it for predictions on the test set and now you have two results: the ground truth **y** that was part of your dataset and the predictions **≈∑** that have just been yielded by your machine learning algorithm. So how do you assess your model in this particular scenario? One way is to build a **confusion matrix**, like the one below:\n",
        "\n",
        "![confusion-matrix](https://glassboxmedicine.files.wordpress.com/2019/02/confusion-matrix.png?w=816)\n",
        "\n",
        "[üåé &nbsp;Image Source](https://glassboxmedicine.com/2019/02/17/measuring-performance-the-confusion-matrix/)\n",
        "\n",
        "Now don‚Äôt get confused (excuse the pun ü§¶. It‚Äôs actually quite simple to interpret it. There are **4 possible predictions** yielded by your binary classifier:\n",
        "\n",
        "  * **True Positives (y = ≈∑ and ≈∑=‚Äùcat‚Äù):**\n",
        "the model identified a cat in the image and in that image it is actually a cat.\n",
        "\n",
        "  * **False Positives (y != ≈∑ and ≈∑=‚Äùcat‚Äù):**\n",
        "the model identified a cat in the image, but there was no cat in the image.\n",
        "\n",
        "  * **False Negatives (y != ≈∑ and ≈∑=‚Äùnon-cat‚Äù):**\n",
        "the model did not recognize a cat in the image, but there was a cat in the image.\n",
        "\n",
        "  * **True Negatives (y = ≈∑ and ≈∑=‚Äùnon-cat‚Äù):**\n",
        "the model did not recognize a cat in the image and in the image there was no cat indeed.\n",
        "\n",
        "---\n",
        "> ### üö®üö®üö® The **first step** in analyzing the performance of a **classifier** is to build a **confusion matrix**!\n",
        "---\n",
        "\n",
        "These 4 scenarios can have different levels of importance, depending on the problem one wants to solve. For a smart antivirus, keeping the number of false negatives as low as possible is crucial, even if that means an increase in the false positives. Naturally, it‚Äôs better to have annoying warnings than having your computer infected because the antivirus was unable to identify the threat. But if you don‚Äôt necessarily have unusual requirements for you model and you just want to assess its generic performance, there are 3 metrics that can be inferred from this matrix:\n",
        "\n",
        "#### Accuracy\n",
        "\n",
        "This is the most intuitive metric and represents the number of correctly predicted labels over the number of total predictions, the actual value of prediction (positive or negative) being irrelevant. Again, judging the model by this single metric can be misleading. Consider the case of credit card transaction frauds. There could be a lot of transactions that are perfectly valid, but only a handful of them are frauds. A model that trains on this data could learn to predict that all of the transactions are safe and the ones that are fraudulent are just outliers. Calculating the accuracy on a dataset of 1 million transactions would yield 99.9%. But our model is useless, because letting 1000 frauds unnoticed cannot be allowed.\n",
        "\n",
        "![accuracy](https://ocw.cs.pub.ro/courses/_media/ep/labs/16._accuracy.png?w=400&tok=dbbf3b)\n",
        "\n",
        "#### Precision\n",
        "\n",
        "The precision is the total number of correctly classified positive examples divided by the total number of predicted positive examples. If the precision is very high, the probability for our model of classifying non-cat images as cat images is quite low.\n",
        "\n",
        "![precision](https://ocw.cs.pub.ro/courses/_media/ep/labs/17._precision.png?w=400&tok=eca58f)\n",
        "\n",
        "#### Recall\n",
        "\n",
        "The recall is the total number of correctly classified positive examples divided by the total number of actual positive examples. If the recall is very high, the probability for our model of misclassifying cat images is quite low.\n",
        "\n",
        "![recall](https://ocw.cs.pub.ro/courses/_media/ep/labs/18._recall.png?w=400&tok=57e5df)\n",
        "\n",
        "When you have **high recall** and **low precision**, most of the cat images are correctly recognized, but there are a lot of false positives. In contrast, when you have **low recall** and **high precision**, we miss a lot of cat images, but those predicted as cat images have a high probability of being indeed cat images and not something else.\n",
        "\n",
        "![confusion-matrix-sketch](https://miro.medium.com/max/1854/1*uR09zTlPgIj5PvMYJZScVg.png)\n",
        "\n",
        "[üåé &nbsp;Image Source](https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62)\n",
        "\n",
        "#### F1 Score\n",
        "\n",
        "Ideally, you would want to have both **high recall** and **high precision**, but that is not always possible. So you can choose the trade-off you are most comfortable with or you can combine the 2 metrics into 1, by using the **F1 score**, which is actually the harmonic mean of the 2:\n",
        "\n",
        "![f1-score](https://ocw.cs.pub.ro/courses/_media/ep/labs/19._f1_score.png?w=400&tok=1b9b39)\n",
        "\n",
        "This single metric is more generic, goes from **0** (worst) to **1** (best) and together with accuracy, can give you a solid intuition on the performance of your model.\n",
        "\n",
        "---\n",
        "> ### üö®üö®üö® From the **confusion matrix** you can extract the **accuracy**, **precision** and **recall**!\n",
        "---\n",
        "\n",
        "#### Generic Confusion Matrix\n",
        "\n",
        "Don‚Äôt take the terms ‚Äúpositive‚Äù and ‚Äúnegative‚Äù written in the confusion matrix above literally! They are actually placeholders for the 2 classes (‚Äúcat‚Äù and ‚Äúnon-cat‚Äù) the model is trying to predict. This means that you can use more generic classes like ‚Äúcat‚Äù and ‚Äúdog‚Äù or ‚Äúhuman‚Äù and ‚Äúanimal‚Äù, depending on the problem you want to solve. As a consequence, **each metric** (accuracy, precision, recall or F1 score) is computed **per class** and if you want to obtain a **generic score** for the model, you must compute their **average**. For instance, if you have a recall of **0.6** for \"**cat**\" and **0.5** for \"**dog**\", the **average recall** of the model will be **0.55**.\n",
        "\n",
        "Consequently, the confusion matrix can be generalized to more than 2 classes (see the image below), having the same metrics and ways of computing them.\n",
        "\n",
        "![generic-confusion-matrix](https://res.cloudinary.com/practicaldev/image/fetch/s--cRGF5_MX--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://thepracticaldev.s3.amazonaws.com/i/hri4evvbu1t2v7mmmvct.png)\n",
        "\n",
        "[üåé &nbsp;Image Source](https://dev.to/overrideveloper/understanding-the-confusion-matrix-264i)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4x2EA9fnBL1"
      },
      "source": [
        "### C. Regression Problems\n",
        "\n",
        "As it was previously mentioned, **supervised machine learning** can also imply solving **regression tasks**, where a **numerical** or **continuous** value is used as a label. Let‚Äôs take an example. Suppose that you want to predict the budget for an advertisement campaign, based on the revenue of the company. For this task, your model will learn from a training set of (X, y) pairs and will try to find the best **ƒ•** that approximates the behaviour of the function **f(X) = y**. If we assume that the relationship between the two is **linear**, your model must learn how to **draw a line** that fits the points (X, y) the best. A visual representation can be seen in the figure below:\n",
        "\n",
        "![linear-regression](https://stackabuse.s3.amazonaws.com/media/multiple-linear-regression-with-python-1.png)\n",
        "\n",
        "[üåé &nbsp;Image Source](https://stackabuse.com/multiple-linear-regression-with-python/)\n",
        "\n",
        "#### Root Mean Squared Error\n",
        "\n",
        "But how do we know if that line is drawn correctly? Well, first we must define an **error** or **loss function** that can mathematically indicate us how far from the points the line has been drawn. In linear regression tasks, the most common approach is to use the **root mean squared error (RMSE)**, depicted below. You can sometimes get rid of the root and just compute the **MSE** to avoid extra computations.\n",
        "\n",
        "![rmse](https://ocw.cs.pub.ro/courses/_media/ep/labs/20._rmse.png?w=400&tok=535cdc)\n",
        "\n",
        "In this formula, **y<sub>j</sub>** is the ground truth and **≈∑<sub>j</sub>** is the result of the partially learned function **ƒ•(X<sub>j</sub>)**. Basically, the model must apply successive corrections to ƒ•, such that the predicted ≈∑<sub>j</sub> values lead to a smaller RMSE. This process of minimizing the loss is also called **optimization** and is one of the foundational principles of machine learning. You can find more about it [here](https://towardsdatascience.com/linear-regression-using-gradient-descent-97a6c8700931).\n",
        "\n",
        "Similarly to the classification tasks, for regression problems, the value of the RMSE can be used as a **performance metric** for the model.\n",
        "\n",
        "#### R¬≤ Correlation\n",
        "\n",
        "Now let‚Äôs say that your model was properly trained and you have some predicted labels **≈∑** and some ground truth values **y**. In the case of classification problems, with these two pieces of information you could immediately compute the F1 score or the accuracy. But with regression, a 0 to 1 score cannot be simply derived. One metric that can be used however, is the **R<sup>2</sup> correlation** and it is computed using the following formula:\n",
        "\n",
        "![r2-correlation](https://ocw.cs.pub.ro/courses/_media/ep/labs/21._r-squared_score.png?w=400&tok=30c26d)\n",
        "\n",
        "where **≈∑<sub>j</sub>** is the predicted value, **√ø** is the mean of the ground truth labels and **y<sub>j</sub>** is the ground truth. This score lies between **-‚àû** and **1** and has the following interpretation:\n",
        "\n",
        "  * **Close to 1:** high positive linear correlation between X and y\n",
        "  * **Close to 0:** a linear correlation between X and y cannot be identified\n",
        "  * **Close to -‚àû:** high negative linear correlation between X and y\n",
        "\n",
        "These 3 scenarios are visually represented in the figures below:\n",
        "\n",
        "![correlation-types](https://statistics.laerd.com/statistical-guides/img/pearson-1-small.png)\n",
        "\n",
        "[üåé &nbsp;Image Source](https://statistics.laerd.com/stata-tutorials/pearsons-correlation-using-stata.php)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trKKzpYuusfa"
      },
      "source": [
        "---\n",
        "> ### üö®üö®üö® For **regression tasks** you can use **RMSE** and **R-squared score** as performance metrics!\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSevKrJ_okWi"
      },
      "source": [
        "### D. Underfitting vs Overfitting\n",
        "\n",
        "Splitting the data into a **training set** and a **test set** is not only helping us to determine the **accuracy** or **error** of the model‚Äôs predictions, but can also give us an insight about its **behaviour** or **generalization capabilities**.\n",
        "\n",
        "Let‚Äôs take a trained binary classifier as an example and the accuracy as a performance metric. If it has a **small test score**, but a **high training score**, the model relied too much on the training data and is not able to generalize well on entries that has never seen before. In this case, we say that it has **overfit** the training data. In this particular case, the model is said to have a **high variance**. In contrast, if the model learned a function that is too generic, the problem of **underfitting** or **high bias** occurs. This time, the problem can be inferred from a **training score** that is **too small**. These situations were visually represented in the figures below:\n",
        "\n",
        "![fitting](https://miro.medium.com/max/1400/1*JZbxrdzabrT33Yl-LrmShw.png)\n",
        "\n",
        "[üåé &nbsp;Image Source](https://medium.com/greyatom/what-is-underfitting-and-overfitting-in-machine-learning-and-how-to-deal-with-it-6803a989c76)\n",
        "\n",
        "The underlying causes of the aforementioned problems depend heavily on the model and how its **hyperparameters** were fine-tuned. Discussing them is outside the scope of this laboratory class, but one can learn more about those topics from these articles: [Underfitting & Overfitting](https://towardsdatascience.com/what-are-overfitting-and-underfitting-in-machine-learning-a96b30864690) and [Hyperparameters](https://towardsdatascience.com/understanding-hyperparameters-and-its-optimisation-techniques-f0debba07568)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xl3ZolRrvEg1"
      },
      "source": [
        "---\n",
        "> ### üö®üö®üö® Besides looking at the usual **performance metrics**, one must also notice if the model **appropriately fits** the data!\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cO3kOM-EpVCs"
      },
      "source": [
        "### E. Clustering Algorithms\n",
        "\n",
        "In the end, we should talk a little bit about **unsupervised learning**. As you already know, for such tasks, there is **no ground truth** - just a set of X values that might have some **underlying structure** or **pattern** that can be learned. Evaluating a model without having something as reference might seem a pretty difficult task. And in some scenarios, you might be right!\n",
        "\n",
        "Say that we wanted to group images containing handwritten digits into **clusters** and at the end of the learning process, our model grouped the data like this:\n",
        "\n",
        "![digit-clustering](https://miro.medium.com/max/3200/1*R7A0qNVuOjgGh6pj4WUq5A.png)\n",
        "\n",
        "[üåé &nbsp;Image Source](https://towardsdatascience.com/graduating-in-gans-going-from-understanding-generative-adversarial-networks-to-running-your-own-39804c283399)\n",
        "\n",
        "At first glance, the clustering outcome looks good, but how can we express this ‚Äúgood-looking‚Äù result in a more formal manner? One solution is to measure how compact the clusters are, yet distant from one another, by computing a **silhouette score**. Because the formulas are too cumbersome to write in here, I will leave you a [link](https://en.wikipedia.org/wiki/Silhouette_(clustering)), where everything is explained clearly. However, one might understand the concept by looking at this simple example:\n",
        "\n",
        "![silhouette-score](https://lh4.googleusercontent.com/-ku6hE_438mo/TXA0yYUWLTI/AAAAAAAAACs/UqtvwlXLkc0/s320/picture2.gif)\n",
        "\n",
        "[üåé &nbsp;Image Source](http://blog.data-miners.com/2011/03/cluster-silhouettes.html)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d47Gl2yeksCZ"
      },
      "source": [
        "**Exercise 0: Prerequisite**\n",
        "\n",
        "Import the necessary libraries and define some printing functions for later use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlI5jmW3XnTr"
      },
      "source": [
        "# Import the necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.datasets import make_blobs"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncBYKmhFaZJs"
      },
      "source": [
        "# Load various printing functions\n",
        "\n",
        "# Pretty prints a data frame without display limits\n",
        "def print_df(df):\n",
        "    with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
        "        print(df)\n",
        "\n",
        "\n",
        "# Pretty prints the results of the classifier performance evaluation\n",
        "def print_classifier_results(accuracy, precision, recall, f1, mode):\n",
        "    print()\n",
        "    print('~~~~~~~~~~~~~~~~ CLASSIFICATION RESULTS (' + mode.upper() + ') ~~~~~~~~~~~~~~~~')\n",
        "    print(' Accuracy: ' + str(accuracy))\n",
        "    print('Precision: ' + str(precision))\n",
        "    print('   Recall: ' + str(recall))\n",
        "    print(' F1 Score: ' + str(f1))\n",
        "\n",
        "\n",
        "# Pretty prints the results of the regressor performance evaluation\n",
        "def print_regressor_results(rmse, mae, r_squared, mode):\n",
        "    print()\n",
        "    print('~~~~~~~~~~~~~~~~ REGRESSION RESULTS (' + mode.upper() + ') ~~~~~~~~~~~~~~~~')\n",
        "    print('Root Mean Squared Error (RMSE): ' + str(rmse))\n",
        "    print('     Mean Absolute Error (MAE): ' + str(mae))\n",
        "    print('               R-squared Score: ' + str(r_squared))\n",
        "\n",
        "\n",
        "# Pretty prints the results of the multiple classifications\n",
        "def print_fitting_results(accuracy_list):\n",
        "\n",
        "    print()\n",
        "    print('~~~~~~~~~~~~~~~~ CLASSIFICATION RESULTS ~~~~~~~~~~~~~~~~')\n",
        "\n",
        "    for accuracy_pair in accuracy_list:\n",
        "        print('Training Set Accuracy 1: ' + str(accuracy_pair[0]))\n",
        "        print('    Test Set Accuracy 1: ' + str(accuracy_pair[1]))\n",
        "        print()\n",
        "\n",
        "\n",
        "# Pretty prints the results of the clustering algorithm\n",
        "def print_clustering_results(silhouette_score):\n",
        "\n",
        "    print()\n",
        "    print('~~~~~~~~~~~~~~~~ CLUSTERING RESULTS ~~~~~~~~~~~~~~~~')\n",
        "    print('Silhouette Score: ' + str(silhouette_score))\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtnzaEOtgxHl"
      },
      "source": [
        "**Exercise 1: Classification (30p)**\n",
        "\n",
        "In this exercise, you will learn how to properly evaluate a **classifier**. We chose a **decision tree** for this example, but feel free to explore other alternatives. You can find out more about decision trees [here](https://towardsdatascience.com/decision-trees-explained-3ec41632ceb6). For all the associated tasks, you will use the [diabetes.csv](https://github.com/vladastefanescu/machine-learning-introduction/blob/main/diabetes.csv) dataset. The model must learn to determine whether the patient suffers from diabetes (**0** or **1**) by looking at a dataset with the following **features**:\n",
        "\n",
        "  * Number of pregnancies\n",
        "  * Glucose level\n",
        "  * Blood pressure\n",
        "  * Skin thickness\n",
        "  * Insulin level\n",
        "  * Body Mass Index (BMI)\n",
        "  * Diabetes pedigree function (likelihood of diabetes based on family history)\n",
        "  * Age"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XK-CVa-Lg1y6",
        "outputId": "b10e2d0f-6d25-4893-f881-8bc1e0f3e8be"
      },
      "source": [
        "# Load the dataset from the local session folder\n",
        "classification_dataset = pd.read_csv(\"https://raw.githubusercontent.com/vladastefanescu/machine-learning-introduction/main/diabetes.csv\")\n",
        "\n",
        "# Take a look at the first entries of the dataset\n",
        "print_df(classification_dataset.head())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
            "0            6      148             72             35        0  33.6   \n",
            "1            1       85             66             29        0  26.6   \n",
            "2            8      183             64              0        0  23.3   \n",
            "3            1       89             66             23       94  28.1   \n",
            "4            0      137             40             35      168  43.1   \n",
            "\n",
            "   DiabetesPedigreeFunction  Age  Outcome  \n",
            "0                     0.627   50        1  \n",
            "1                     0.351   31        0  \n",
            "2                     0.672   32        1  \n",
            "3                     0.167   21        0  \n",
            "4                     2.288   33        1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YikM5ILSiRK5"
      },
      "source": [
        "# Extract the features from the dataset\n",
        "classification_X = classification_dataset.iloc[:, :-1]\n",
        "\n",
        "# Extract the labels from the dataset\n",
        "classification_y = classification_dataset.iloc[:, -1:]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8e-Ev9JRid7F"
      },
      "source": [
        "# Split the data into a training set and a test set with a 80-20 ratio\n",
        "classification_X_train, classification_X_test, classification_y_train, classification_y_test = train_test_split(classification_X, classification_y, test_size=0.2, random_state=42)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTyLuOBGiqxj"
      },
      "source": [
        "# Build a classifier (less important: a decision tree will be used)\n",
        "classifier = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Fit data from the training set to the classifier\n",
        "classifier.fit(classification_X_train, classification_y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "classification_y_pred = classifier.predict(classification_X_test)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0R2wgNpi4Aq"
      },
      "source": [
        "**Task A (15p)**\n",
        "\n",
        "Evaluate the classifier by **manually** computing the **accuracy**, **precision**, **recall** and **F1 score**. These metrics are derived from the **confusion matrix** but you don't have to build this matrix yourself. You can use [Scikit-learn](https://www.google.com/search?q=scikit-learn+confusion+matrix&oq=scikit-learn+confusion+matrix) for that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBVsEPkwi8LL"
      },
      "source": [
        "# Computes the accuracy of the model using the confusion matrix\n",
        "def compute_accuracy(cm):\n",
        "    # TODO - TASK A\n",
        "    accuracy = None\n",
        "\n",
        "    return accuracy"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgXU3QoHjBku"
      },
      "source": [
        "# Computes the precision of the model using the confusion matrix\n",
        "def compute_precision(cm):\n",
        "    # TODO - TASK A\n",
        "    precision = None\n",
        "\n",
        "    return precision"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2fTgY3VjDGl"
      },
      "source": [
        "# Computes the recall of the model using the confusion matrix\n",
        "def compute_recall(cm):\n",
        "    # TODO - TASK A\n",
        "    recall = None\n",
        "\n",
        "    return recall"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQfdlB9TjFfw"
      },
      "source": [
        "# Computes the F1 score of the model using the precision and recall\n",
        "def compute_f1_score(precision, recall):\n",
        "    # TODO - TASK A\n",
        "    f1 = None\n",
        "\n",
        "    return f1"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jp4RE1G70fFn"
      },
      "source": [
        "Before anything else, we must compute the **confusion matrix**. Luckily, the **metrics** package of the *Scikit-learn* library has just the right function for this task:\n",
        "\n",
        "```\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cm[0][0], cm[1][1] = cm[1][1], cm[0][0]\n",
        "```\n",
        "\n",
        "---\n",
        "> #### üö®üö®üö® Please note that in order to compute a binary confusion matrix that looks just like the ones used in the above diagrams, a small swap has to be performed.\n",
        "---\n",
        "\n",
        "For this exercise we simply identify the **TP**, **TN**, **FP**, **FN** terms on the **confusion matrix** and then compute the various metrics using their formulas. For instance, for the **precision**, we can write the following lines of code:\n",
        "\n",
        "```\n",
        "precision = cm[0][0] / (cm[0][0] + cm[0][1])\n",
        "```\n",
        "\n",
        "---\n",
        "> #### üö®üö®üö® You should be able to compute the **accuracy**, **recall** and **F1 score** by yourself.\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDyqs3O2jLku",
        "outputId": "15904407-f7f1-497f-caa1-f7f44a7906ef"
      },
      "source": [
        "# TODO - TASK A\n",
        "cm = None\n",
        "\n",
        "task_a_accuracy = compute_accuracy(cm)\n",
        "task_a_precision = compute_precision(cm)\n",
        "task_a_recall = compute_recall(cm)\n",
        "task_a_f1 = compute_f1_score(task_a_precision, task_a_recall)\n",
        "\n",
        "print_classifier_results(task_a_accuracy, task_a_precision, task_a_recall, task_a_f1, 'dumb')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "~~~~~~~~~~~~~~~~ CLASSIFICATION RESULTS (DUMB) ~~~~~~~~~~~~~~~~\n",
            " Accuracy: None\n",
            "Precision: None\n",
            "   Recall: None\n",
            " F1 Score: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEXMPg8UjdOI"
      },
      "source": [
        "**Task B (15p)**\n",
        "\n",
        "Evaluate the classifier using the **metrics** package from the *Scikit-learn* library. Again, **accuracy**, **precision**, **recall** and **F1 score** are required.\n",
        "\n",
        "---\n",
        "> #### üö®üö®üö® **HINT:** There are 2 *Scikit-learn* functions that will help you with this computation: **accuracy_score** and **precision_recall_fscore_support**. Because we are computing these metrics on a binary classifier, think about what is the suitable value for the **average** parameter of the **precision_recall_fscore_support** function.\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxfOFZFWjfEl",
        "outputId": "d17060a7-c57c-4b7a-d437-64b9598d20d3"
      },
      "source": [
        "# TODO - TASK B\n",
        "task_b_accuracy = None\n",
        "task_b_precision, task_b_recall, task_b_f1, _ = None, None, None, None\n",
        "\n",
        "print_classifier_results(task_b_accuracy, task_b_precision, task_b_recall, task_b_f1, 'smart')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "~~~~~~~~~~~~~~~~ CLASSIFICATION RESULTS (SMART) ~~~~~~~~~~~~~~~~\n",
            " Accuracy: None\n",
            "Precision: None\n",
            "   Recall: None\n",
            " F1 Score: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "du9LS4X1b_H-"
      },
      "source": [
        "**Exercise 2: Linear Regression (40p)**\n",
        "\n",
        "In this exercise, you will learn how to properly evaluate a **regression model**. We chose a **simple linear regressor** for this example, but feel free to explore other alternatives. You can find out more about linear regressors [here](https://www.kdnuggets.com/2019/03/beginners-guide-linear-regression-python-scikit-learn.html). For all the associated tasks, you will use the [weather.csv](https://github.com/vladastefanescu/machine-learning-introduction/blob/main/weather.csv) dataset. The model must learn to determine what is the **maximum temperature** for a certain day (**y**) based on the **minimum temperature** (**X**).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GDNE91bZXlO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d655ef7-371d-4ae5-fdc9-b385e3a033d6"
      },
      "source": [
        "# Load the dataset from the local session folder\n",
        "regression_dataset = pd.read_csv(\"https://raw.githubusercontent.com/vladastefanescu/machine-learning-introduction/main/weather.csv\", dtype=np.float64)\n",
        "\n",
        "# Take a look at the first entries of the dataset\n",
        "print_df(regression_dataset.head())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     MinTemp    MaxTemp\n",
            "0  22.222222  25.555556\n",
            "1  21.666667  28.888889\n",
            "2  22.222222  26.111111\n",
            "3  22.222222  26.666667\n",
            "4  21.666667  26.666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDCUAdnEawwg"
      },
      "source": [
        "# Extract the features from the dataset\n",
        "regression_X = regression_dataset.iloc[:, :-1]\n",
        "\n",
        "# Extract the labels from the dataset\n",
        "regression_y = regression_dataset.iloc[:, -1:]"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBhA_77fbDqT"
      },
      "source": [
        "# Split the data into a training set and a test set with a 80-20 ratio\n",
        "regression_X_train, regression_X_test, regression_y_train, regression_y_test = train_test_split(regression_X, regression_y, test_size=0.2, random_state=42)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhKiXZGbbd-_"
      },
      "source": [
        "# Build a linear regressor\n",
        "regressor = LinearRegression()\n",
        "\n",
        "# Fit data from the training set to the regressor\n",
        "regressor.fit(regression_X_train, regression_y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "regression_y_pred = regressor.predict(regression_X_test)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lRDsGheeB_K"
      },
      "source": [
        "**Task A (15p)**\n",
        "\n",
        "Evaluate the regressor by **manually** computing the **Root Mean Squared Error (RMSE)**, **Mean Absolute Error (MAE)** and **R¬≤ score**.\n",
        "\n",
        "For instance, for the **RMSE** we simply apply the formula above:\n",
        "\n",
        "```\n",
        "rmse = list(np.sqrt(np.sum((y_test - y_pred) ** 2) / len(y_test)))[0]\n",
        "```\n",
        "\n",
        "---\n",
        "> #### üö®üö®üö® You should be able to compute **MAE** and **R¬≤ score** on your own!\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gS__DyPheD0k"
      },
      "source": [
        "# Computes the RMSE of the model\n",
        "def compute_rmse(y_test, y_pred):\n",
        "    # TODO - TASK A\n",
        "    return None"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2n8WJ8Uz3A_z"
      },
      "source": [
        "---\n",
        "> #### üö®üö®üö® We know that **MAE** was not covered in the first section of this laboratory class and that is why you must use your powerful [Google](https://www.google.com/search?q=mean+absolute+error&oq=mean+absolute+error) skills to solve this one. üòä\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlXKzdNheK9D"
      },
      "source": [
        "# Computes the MAE of the model\n",
        "def compute_mae(y_test, y_pred):\n",
        "    # TODO - TASK A\n",
        "    return None"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgBe8auWeRBh"
      },
      "source": [
        "# Computes the R-squared score of the model\n",
        "def compute_r2_score(y_test, y_pred):\n",
        "    # TODO - TASK A\n",
        "    return None"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYe8vromehIT",
        "outputId": "d1c92556-2ac8-482c-c1e8-5b6ffa8c3e82"
      },
      "source": [
        "task_a_rmse = compute_rmse(regression_y_test, regression_y_pred)\n",
        "task_a_mae = compute_mae(regression_y_test, regression_y_pred)\n",
        "task_a_r_squared = compute_r2_score(regression_y_test, regression_y_pred)\n",
        "\n",
        "print_regressor_results(task_a_rmse, task_a_mae, task_a_r_squared, 'dumb')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "~~~~~~~~~~~~~~~~ REGRESSION RESULTS (DUMB) ~~~~~~~~~~~~~~~~\n",
            "Root Mean Squared Error (RMSE): None\n",
            "     Mean Absolute Error (MAE): None\n",
            "               R-squared Score: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoTUTYY_e5DY"
      },
      "source": [
        "**Task B (10p)**\n",
        "\n",
        "Evaluate the regressor using the **metrics** package from the *Scikit-learn* library. Again, **RMSE**, **MAE** and **R¬≤ score** are required.\n",
        "\n",
        "---\n",
        "> #### üö®üö®üö® **HINT:** You might not find a function that computes the actual **RMSE**, but the **MSE**.\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lxpwzu6De65V"
      },
      "source": [
        "# TODO - TASK B\n",
        "task_b_rmse = None\n",
        "task_b_mae = None\n",
        "task_b_r_squared = None"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCGg44NNfc8b",
        "outputId": "4c1af0a6-85c9-471c-daaa-d96c6fca7f77"
      },
      "source": [
        "print_regressor_results(task_b_rmse, task_b_mae, task_b_r_squared, 'smart')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "~~~~~~~~~~~~~~~~ REGRESSION RESULTS (SMART) ~~~~~~~~~~~~~~~~\n",
            "Root Mean Squared Error (RMSE): None\n",
            "     Mean Absolute Error (MAE): None\n",
            "               R-squared Score: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8U4WIS-fngT"
      },
      "source": [
        "**Task C (15p)**\n",
        "\n",
        "Train the model on variously sized chunks of the original dataset and notice how the **RMSE** changes. To better illustrate the behaviour, you should build a **plot** having the **data size** on the **X axis** and the **RMSE value** on the **Y axis**. Moreover, you should be able to **explain** the observed behaviour to the assistant.\n",
        "\n",
        "Because this is rather a plotting task, we will do the hard work for you and compute the list of RMSE values for the various chunk sizes:\n",
        "\n",
        "```\n",
        "n = min(X.shape[0], max_chunk_size)\n",
        "chunk_size = int((n - min_chunk_size) / chunks)\n",
        "\n",
        "# Create 2 lists used in the plotting logic\n",
        "size_list = []\n",
        "rmse_list = []\n",
        "\n",
        "# Train a model for each chunk\n",
        "for i in range(0, chunks):\n",
        "\n",
        "    # Compute the size of the current chunk\n",
        "    size = min_chunk_size + (i + 1) * chunk_size\n",
        "    size_list.append(size)\n",
        "\n",
        "    # Select a chunk from the whole dataset\n",
        "    sample_X = X.sample(n=size, random_state=42)\n",
        "    sample_y = y.sample(n=size, random_state=42)\n",
        "\n",
        "    # Split the data into a training set and a test set with a 80-20 ratio\n",
        "    X_train, X_test, y_train, y_test = train_test_split(sample_X, sample_y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Build a linear regressor\n",
        "    regressor = LinearRegression()\n",
        "\n",
        "    # Fit data from the training set to the regressor\n",
        "    regressor.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred = regressor.predict(X_test)\n",
        "\n",
        "    # Compute the rmse\n",
        "    rmse = compute_rmse(y_test, y_pred)\n",
        "    rmse_list.append(rmse)\n",
        "```\n",
        "\n",
        "Please read the code and try to understand it by following the comments. Building the plot is on you! üòä"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2I4toYvffzP8"
      },
      "source": [
        "# Plots the evolution of the RMSE when the dataset size varies\n",
        "def plot_rmse_evolution(X, y, chunks, min_chunk_size, max_chunk_size):\n",
        "    # TODO - TASC C\n",
        "    # HINT: See the code above\n",
        "    # Build the list of RMSE values\n",
        "\n",
        "    # TODO - TASK C\n",
        "    # Plot the RMSE evolution\n",
        "    pass"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPl0A44T4Xtl"
      },
      "source": [
        "Play with the following parameters and observe how the plot changes. Make **at least 5** changes to the chunk-related parameters. That means that **5 plots should be generated**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdph29_9gLzQ"
      },
      "source": [
        "# TODO - TASK C\n",
        "CHUNKS = 100\n",
        "MIN_CHUNK_SIZE = 1000\n",
        "MAX_CHUNK_SIZE = 1000000\n",
        "\n",
        "plot_rmse_evolution(regression_X, regression_y, CHUNKS, MIN_CHUNK_SIZE, MAX_CHUNK_SIZE)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywIb1viTl0jl"
      },
      "source": [
        "**Exercise 3: Fitting Behaviour (15p)**\n",
        "\n",
        "In this exercise, you will learn how to properly evaluate the **data fitting behaviour** of the **binary classifier** from **exercise 1**. For all the associated tasks, you will use the [diabetes.csv](https://github.com/vladastefanescu/machine-learning-introduction/blob/main/diabetes.csv) dataset again. This time, **3 models** will be trained with different parameters and your job is to analyse their behaviour. For that, you will look at the **accuracy** of each model computed on both the **training set** and the **test set**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rg4xLH0l3h9",
        "outputId": "12fc0826-e24d-4a02-e698-418832f1ed5b"
      },
      "source": [
        "# Load the dataset from the local session folder\n",
        "fitting_dataset = pd.read_csv(\"https://raw.githubusercontent.com/vladastefanescu/machine-learning-introduction/main/diabetes.csv\")\n",
        "\n",
        "# Take a look at the first entries of the dataset\n",
        "print_df(fitting_dataset.head())"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
            "0            6      148             72             35        0  33.6   \n",
            "1            1       85             66             29        0  26.6   \n",
            "2            8      183             64              0        0  23.3   \n",
            "3            1       89             66             23       94  28.1   \n",
            "4            0      137             40             35      168  43.1   \n",
            "\n",
            "   DiabetesPedigreeFunction  Age  Outcome  \n",
            "0                     0.627   50        1  \n",
            "1                     0.351   31        0  \n",
            "2                     0.672   32        1  \n",
            "3                     0.167   21        0  \n",
            "4                     2.288   33        1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exi9ikYnmIV9"
      },
      "source": [
        "# Extract the features from the dataset\n",
        "fitting_X = fitting_dataset.iloc[:, :-1]\n",
        "\n",
        "# Extract the labels from the dataset\n",
        "fitting_y = fitting_dataset.iloc[:, -1:]"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9nlVoJjmYUP"
      },
      "source": [
        "# Split the data into a training set and a test set with a 80-20 ratio\n",
        "fitting_X_train, fitting_X_test, fitting_y_train, fitting_y_test = train_test_split(fitting_X, fitting_y, test_size=0.2, random_state=42)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wm_vksG6mnzd"
      },
      "source": [
        "# Build 3 different trained models\n",
        "classifiers = [DecisionTreeClassifier(max_depth=1, random_state=42).fit(fitting_X_train, fitting_y_train),\n",
        "               DecisionTreeClassifier(max_depth=5, random_state=42).fit(fitting_X_train, fitting_y_train),\n",
        "               DecisionTreeClassifier(max_depth=32, random_state=42).fit(fitting_X_train, fitting_y_train)]"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zt2i2F5QnLK_"
      },
      "source": [
        "**Task A (10p)**\n",
        "\n",
        "For each model, make predictions on both the **training set** and **test set** and compute the corresponding **accuracy values**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjS8fh1mnPYA"
      },
      "source": [
        "# Makes prediction on both the training set and test set\n",
        "# HINT 1: You can reuse some code from the previous exercises\n",
        "# HINT 2: The models are already trained\n",
        "def make_predictions(clf, X_train, X_test, y_train, y_test):\n",
        "    # TODO - TASK A\n",
        "    train_accuracy = None\n",
        "    test_accuracy = None\n",
        "\n",
        "    return train_accuracy, test_accuracy"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Dh0prrgm-Nq",
        "outputId": "17c064f5-55e8-432b-df4a-7ffd64a9020c"
      },
      "source": [
        "# Make predictions on the training and test sets and evaluate the performance of each model\n",
        "accuracy_list = []\n",
        "for clf in classifiers:\n",
        "    accuracy_list.append(make_predictions(clf, fitting_X_train, fitting_X_test, fitting_y_train, fitting_y_test))\n",
        "\n",
        "# Print the performance evaluation results\n",
        "print_fitting_results(accuracy_list)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "~~~~~~~~~~~~~~~~ CLASSIFICATION RESULTS ~~~~~~~~~~~~~~~~\n",
            "Training Set Accuracy 1: None\n",
            "    Test Set Accuracy 1: None\n",
            "\n",
            "Training Set Accuracy 1: None\n",
            "    Test Set Accuracy 1: None\n",
            "\n",
            "Training Set Accuracy 1: None\n",
            "    Test Set Accuracy 1: None\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMVq5V7rne_o"
      },
      "source": [
        "**Task B (5p)**\n",
        "\n",
        "Comment the results by specifying which is the **best model** in terms of fitting and which are the models that **overfit** or **underfit** the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WTiKPCSOniVj",
        "outputId": "7189de29-2e29-4475-819d-045571828d68"
      },
      "source": [
        "\"\"\"\n",
        "TODO - TASK B\n",
        "\n",
        "Your conclusion here...\n",
        "\"\"\""
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nTODO - TASK B\\n\\nYour conclusion here...\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGpHojJxn5IT"
      },
      "source": [
        "**Exercise 4: Clustering (15p)**\n",
        "\n",
        "\n",
        "In this exercise, you will learn how to properly evaluate a **clustering model**. We chose a **K-means clustering algorithm** for this example, but feel free to explore other alternatives. You can find out more about K-means clustering algorithms [here](https://towardsdatascience.com/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1). For all the associated tasks, you don't have to use any input dataset, because the clusters are generated in the skeleton. The model must learn how to group together **points in a 2D space**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlpxT-zyn7i_"
      },
      "source": [
        "# NOTE: You can play around with these values\n",
        "CLUSTERS = 4\n",
        "SAMPLES = 300\n",
        "CLUSTERS_STD = 2\n",
        "\n",
        "# Generate a dataset\n",
        "clustering_X, _ = make_blobs(n_samples=SAMPLES, centers=CLUSTERS, cluster_std=CLUSTERS_STD, random_state=13)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "K3nbWrscpUHY",
        "outputId": "dc6c0c54-66e1-45a4-dd61-03b7ff6a229d"
      },
      "source": [
        "# Plot the freshly generated blobs\n",
        "plt.scatter(clustering_X[:, 0], clustering_X[:, 1], s=50)\n",
        "plt.show()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J36-gLd0pjgq"
      },
      "source": [
        "# Create a clustering model (less important: a K-means clustering algorithm will be used)\n",
        "clustering_model = KMeans(n_clusters=CLUSTERS, random_state=13)\n",
        "\n",
        "# Fit the data\n",
        "clustering_model.fit(clustering_X)\n",
        "\n",
        "# Predict a cluster number for each point\n",
        "clustering_y_pred = clustering_model.predict(clustering_X)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRosTyMlp6ad"
      },
      "source": [
        "Task A (5p)\n",
        "\n",
        "Compute the **silhouette score** of the model by using a *Scikit-learn* function found in the **metrics** package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCKz4wLep9Ps",
        "outputId": "360fdaf7-135d-40cf-b8ab-7647bf1f49f4"
      },
      "source": [
        "# Compute the silhouette score of the clusters and print it\n",
        "# TODO - TASK A\n",
        "clustering_score = None\n",
        "print_clustering_results(clustering_score)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "~~~~~~~~~~~~~~~~ CLUSTERING RESULTS ~~~~~~~~~~~~~~~~\n",
            "Silhouette Score: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMTbmHxZqHsL"
      },
      "source": [
        "Task B (10p)\n",
        "\n",
        "Fetch the **centres of the clusters** (the model should already have them ready for you üòä) and **plot** them together with a **colourful 2D representation** of the data groups. Your plot should look similar to the one below:\n",
        "\n",
        "![cluster-centers](https://ocw.cs.pub.ro/courses/_media/ep/labs/22._clustering_plot.png?w=600&tok=262dc7)\n",
        "\n",
        "You can also play around with the **standard deviation** of the generated blobs and observe the different outcomes of the clustering algorithm:\n",
        "\n",
        "```\n",
        "CLUSTERS_STD = 2\n",
        "```\n",
        "\n",
        "You should be able to discuss these observations with the assistant.\n",
        "\n",
        "---\n",
        "> #### üö®üö®üö® **HINT:** The **plotting code** is very similar to the one found in the skeleton. You can also [Google](https://www.google.com/search?q=plot+k+means+cluster+python) it out. üòâ.\n",
        "---\n",
        "\n",
        "Look at the hint above and solve the tasks marked with **TODO - TASK B**. Make **at least 3** changes to the standard deviation. That means that **3 plots should be generated**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCriDUzpqQqS"
      },
      "source": [
        "# Fetch the centers of the clusters\n",
        "# TODO - TASK B\n",
        "\n",
        "# Pretty print the resulting clusters together with their centers\n",
        "# TODO - TASK B"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsMiJ-Pdgjpj"
      },
      "source": [
        "**Bonus: Feedback (10p)**\n",
        "\n",
        "Please take a minute to fill in the [feedback form](https://forms.gle/KHMVUhNfCPoR71Ew7) for this lab."
      ]
    }
  ]
}
